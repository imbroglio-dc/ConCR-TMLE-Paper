
@inproceedings{benkeser_highly_2016,
	title = {The {Highly} {Adaptive} {Lasso} {Estimator}},
	doi = {10.1109/DSAA.2016.93},
	abstract = {Estimation of a regression functions is a common goal of statistical learning. We propose a novel nonparametric regression estimator that, in contrast to many existing methods, does not rely on local smoothness assumptions nor is it constructed using local smoothing techniques. Instead, our estimator respects global smoothness constraints by virtue of falling in a class of right-hand continuous functions with left-hand limits that have variation norm bounded by a constant. Using empirical process theory, we establish a fast minimal rate of convergence of our proposed estimator and illustrate how such an estimator can be constructed using standard software. In simulations, we show that the finite-sample performance of our estimator is competitive with other popular machine learning techniques across a variety of data generating mechanisms. We also illustrate competitive performance in real data examples using several publicly available data sets.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Benkeser, David and Van Der Laan, Mark},
	month = oct,
	year = {2016},
	note = {ISSN: null},
	keywords = {Convergence, finite-sample performance, highly adaptive lasso estimator, Kernel, lasso, learning (artificial intelligence), local smoothing techniques, machine learning techniques, Manganese, Maximum likelihood estimation, nonparametric regression, novel nonparametric regression estimator, prediction, regression analysis, regression functions, Standards, statistical learning},
	pages = {689--696},
	file = {IEEE Xplore Abstract Record:/home/imbroglio/Zotero/storage/2E8GWI5P/7796956.html:text/html;IEEE Xplore Full Text PDF:/home/imbroglio/Zotero/storage/W36YLWAB/Benkeser and Van Der Laan - 2016 - The Highly Adaptive Lasso Estimator.pdf:application/pdf},
}

@article{laan_generally_2017,
	title = {A {Generally} {Efficient} {Targeted} {Minimum} {Loss} {Based} {Estimator} based on the {Highly} {Adaptive} {Lasso}},
	volume = {13},
	doi = {10.1515/ijb-2015-0097},
	language = {en},
	number = {2},
	journal = {The International Journal of Biostatistics},
	author = {Laan, Mark J. van der},
	month = oct,
	year = {2017},
	keywords = {asymptotic linear estimator, canonical gradient, cross-validated targeted minimum loss estimation (CV-TMLE), Donsker class, efficient estimator, efficient influence curve, empirical process, entropy, highly adaptive Lasso, influence curve, one-step TMLE, super-learning, targeted minimum loss estimation (TMLE)},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/XKL3MEPA/Laan - 2017 - A Generally Efficient Targeted Minimum Loss Based .pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/JDTQGV9M/van - 2017 - A Generally Efficient Targeted Minimum Loss Based .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/XS5VA3KS/article-20150097.html:text/html},
}

@article{laan_unified_2003,
	title = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}: {Finite} {Sample} {Oracle} {Inequalities} and {Examples}},
	shorttitle = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {Laan, Mark J. van der and Dudoit, Sandrine},
	month = nov,
	year = {2003},
	file = {"Unified Cross-Validation Methodology For Selection Among Estimators an" by Mark J. van der Laan and Sandrine Dudoit:/home/imbroglio/Zotero/storage/2LM57J99/paper130.html:text/html},
}

@article{laan_targeted_2006,
	title = {Targeted {Maximum} {Likelihood} {Learning}},
	volume = {2},
	issn = {1557-4679},
	doi = {10.2202/1557-4679.1043},
	language = {en},
	number = {1},
	journal = {The International Journal of Biostatistics},
	author = {Laan, Mark J. van der and Rubin, Daniel},
	month = dec,
	year = {2006},
	note = {Publisher: De Gruyter
Section: The International Journal of Biostatistics},
	keywords = {causal effect, cross-validation, efficient influence curve, estimating function, locally efficient estimation, loss function, maximum likelihood estimation, sieve, targeted maximum likelihood estimation, variable importance},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/7TYKLA68/Laan and Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/HW2CYHB3/van and Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/AW9TSIUM/article-ijb.2006.2.1.1043.xml.html:text/html},
}

@book{bickel_efficient_1998,
	address = {New York},
	title = {Efficient and {Adaptive} {Estimation} for {Semiparametric} {Models}},
	isbn = {978-0-387-98473-5},
	abstract = {This book is about estimation in situations where we believe we have enough knowledge to model some features of the data parametrically, but are unwilling to assume anything for other features. Such models have arisen in a wide variety of contexts in recent years, particularly in economics, epidemiology, and astronomy. The complicated structure of these models typically requires us to consider nonlinear estimation procedures which often can only be implemented algorithmically. The theory of these procedures is necessarily based on asymptotic approximations.},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Bickel, Peter J. and Klaassen, Chris A. J. and Ritov, Ya'acov and Wellner, Jon A.},
	year = {1998},
	file = {Snapshot:/home/imbroglio/Zotero/storage/ZRB54V8R/9780387984735.html:text/html},
}

@book{laan_unified_2003-1,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Unified {Methods} for {Censored} {Longitudinal} {Data} and {Causality}},
	isbn = {978-0-387-95556-8},
	abstract = {During the last decades, there has been an explosion in computation and information technology. This development comes with an expansion of complex observational studies and clinical trials in a variety of fields such as medicine, biology, epidemiology, sociology, and economics among many others, which involve collection of large amounts of data on subjects or organisms over time. The goal of such studies can be formulated as estimation of a finite dimensional parameter of the population distribution corresponding to the observed time- dependent process. Such estimation problems arise in survival analysis, causal inference and regression analysis. This book provides a fundamental statistical framework for the analysis of complex longitudinal data. It provides the first comprehensive description of optimal estimation techniques based on time-dependent data structures subject to informative censoring and treatment assignment in so called semiparametric models. Semiparametric models are particularly attractive since they allow the presence of large unmodeled nuisance parameters. These techniques include estimation of regression parameters in the familiar (multivariate) generalized linear regression and multiplicative intensity models. They go beyond standard statistical approaches by incorporating all the observed data to allow for informative censoring, to obtain maximal efficiency, and by developing estimators of causal effects. It can be used to teach masters and Ph.D. students in biostatistics and statistics and is suitable for researchers in statistics with a strong interest in the analysis of complex longitudinal data.},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Laan, Mark J. van der and Robins, James M.},
	year = {2003},
	file = {Snapshot:/home/imbroglio/Zotero/storage/KYJPZAEA/9780387955568.html:text/html},
}

@article{benkeser_improved_2018,
	title = {Improved estimation of the cumulative incidence of rare outcomes},
	volume = {37},
	issn = {02776715},
	doi = {10.1002/sim.7337},
	language = {en},
	number = {2},
	journal = {Statistics in Medicine},
	author = {Benkeser, David and Carone, Marco and Gilbert, Peter B.},
	month = jan,
	year = {2018},
	pages = {280--293},
	file = {Benkeser et al. - 2018 - Improved estimation of the cumulative incidence of.pdf:/home/imbroglio/Zotero/storage/RXIYF8YW/Benkeser et al. - 2018 - Improved estimation of the cumulative incidence of.pdf:application/pdf},
}

@article{laan_super_2007,
	title = {Super {Learner}},
	volume = {6},
	issn = {1544-6115, 2194-6302},
	doi = {10.2202/1544-6115.1309},
	language = {en},
	number = {1},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Laan, Mark J. van der and Polley, Eric C. and Hubbard, Alan E.},
	month = sep,
	year = {2007},
	keywords = {cross-validation, loss-based estimation, machine learning, prediction},
	file = {:/home/imbroglio/Zotero/storage/UAUGVHQ8/paper222.html:text/html;Full Text PDF:/home/imbroglio/Zotero/storage/ENKNI7VY/Laan et al. - 2007 - Super Learner.pdf:application/pdf;Full Text PDF:/home/imbroglio/Zotero/storage/GAN8VE4A/van et al. - 2007 - Super Learner.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/RT2KNC4V/article-sagmb.2007.6.1.1309.xml.html:text/html},
}

@article{petersen_diagnosing_2012,
	title = {Diagnosing and responding to violations in the positivity assumption},
	volume = {21},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280210386207},
	doi = {10.1177/0962280210386207},
	language = {en},
	number = {1},
	urldate = {2020-11-03},
	journal = {Statistical Methods in Medical Research},
	author = {Petersen, Maya L and Porter, Kristin E and Gruber, Susan and Wang, Yue and van der Laan, Mark J},
	month = feb,
	year = {2012},
	pages = {31--54},
	file = {Full Text:/home/imbroglio/Zotero/storage/U7WPTWMY/Petersen et al. - 2012 - Diagnosing and responding to violations in the pos.pdf:application/pdf},
}

@article{vaart_oracle_2006,
	title = {Oracle inequalities for multi-fold cross validation},
	volume = {24},
	issn = {0721-2631},
	doi = {10.1524/stnd.2006.24.3.351},
	abstract = {We consider choosing an estimator or model from a given class by cross validation consisting of holding a nonneglible fraction of the observations out as a test set. We derive bounds that show that the risk of the resulting procedure is (up to a constant) smaller than the risk of an oracle plus an error which typically grows logarithmically with the number of estimators in the class. We extend the results to penalized cross validation in order to control unbounded loss functions. Applications include regression with squared and absolute deviation loss and classiﬁcation under Tsybakov’s condition.},
	language = {en},
	number = {3},
	journal = {Statistics \& Decisions},
	author = {Vaart, Aad W. van der and Dudoit, Sandrine and Laan, Mark J. van der},
	month = jan,
	year = {2006},
	file = {Vaart et al. - 2006 - Oracle inequalities for multi-fold cross validatio.pdf:/home/imbroglio/Zotero/storage/EK4UN6IF/Vaart et al. - 2006 - Oracle inequalities for multi-fold cross validatio.pdf:application/pdf},
}

@book{laan_targeted_2011,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	isbn = {978-1-4419-9781-4},
	shorttitle = {Targeted {Learning}},
	abstract = {The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, censored data, longitudinal data, and genomic studies."Targeted Learning, by Mark J. van der Laan and Sherri Rose, fills a much needed gap in statistical and causal inference. It protects us from wasting computational, analytical, and data resources on irrelevant aspects of a problem and teaches us how to focus on what is relevant – answering questions that researchers truly care about."-Judea Pearl, Computer Science Department, University of California, Los Angeles"In summary, this book should be on the shelf of every investigator who conducts observational research and randomized controlled trials. The concepts and methodology are foundational for causal inference and at the same time stay true to what the data at hand can say about the questions that motivate their collection."-Ira B. Tager, Division of Epidemiology, University of California, Berkeley},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Laan, Mark J. van der and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1},
	file = {Snapshot:/home/imbroglio/Zotero/storage/FSIGDFF3/9781441997814.html:text/html;van der Laan and Rose - 2011 - Targeted Learning.pdf:/home/imbroglio/Zotero/storage/PY8TRAXJ/van der Laan and Rose - 2011 - Targeted Learning.pdf:application/pdf},
}

@book{laan_targeted_2018,
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Learning} in {Data} {Science}: {Causal} {Inference} for {Complex} {Longitudinal} {Studies}},
	isbn = {978-3-319-65303-7},
	shorttitle = {Targeted {Learning} in {Data} {Science}},
	abstract = {This textbook for graduate students in statistics, data science, and public health deals with the practical challenges that come with big, complex, and dynamic data. It presents a scientific roadmap to translate real-world data science applications into formal statistical estimation problems by using the general template of targeted maximum likelihood estimators. These targeted machine learning algorithms estimate quantities of interest while still providing valid inference. Targeted learning methods within data science area critical component for solving scientific problems in the modern age. The techniques can answer complex questions including optimal rules for assigning treatment based on longitudinal data with time-dependent confounding, as well as other estimands in dependent data structures, such as networks. Included in Targeted Learning in Data Science are demonstrations with soft ware packages and real data sets that present a case that targeted learning is crucial for the next generation of statisticians and data scientists. Th is book is a sequel to the first textbook on machine learning for causal inference, Targeted Learning, published in 2011.Mark van der Laan, PhD, is Jiann-Ping Hsu/Karl E. Peace Professor of Biostatistics and Statistics at UC Berkeley. His research interests include statistical methods in genomics, survival analysis, censored data, machine learning, semiparametric models, causal inference, and targeted learning. Dr. van der Laan received the 2004 Mortimer Spiegelman Award, the 2005 Van Dantzig Award, the 2005 COPSS Snedecor Award, the 2005 COPSS Presidential Award, and has graduated over 40 PhD students in biostatistics and statistics.Sherri Rose, PhD, is Associate Professor of Health Care Policy (Biostatistics) at Harvard Medical School. Her work is centered on developing and integrating innovative statistical approaches to advance human health. Dr. Rose’s methodological research focuses on nonparametric machine learning for causal inference and prediction. She co-leads the Health Policy Data Science Lab and currently serves as an associate editor for the Journal of the American Statistical Association and Biostatistics.},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Laan, Mark J. van der and Rose, Sherri},
	year = {2018},
	doi = {10.1007/978-3-319-65304-4},
	file = {Snapshot:/home/imbroglio/Zotero/storage/RYXHVNK8/9783319653037.html:text/html;Snapshot:/home/imbroglio/Zotero/storage/DNGAFAFR/9783319653037.html:text/html},
}

@article{petersen_causal_2014,
	title = {Causal models and learning from data: integrating causal modeling and statistical estimation},
	volume = {25},
	issn = {1531-5487},
	shorttitle = {Causal models and learning from data},
	doi = {10.1097/EDE.0000000000000078},
	abstract = {The practice of epidemiology requires asking causal questions. Formal frameworks for causal inference developed over the past decades have the potential to improve the rigor of this process. However, the appropriate role for formal causal thinking in applied epidemiology remains a matter of debate. We argue that a formal causal framework can help in designing a statistical analysis that comes as close as possible to answering the motivating causal question, while making clear what assumptions are required to endow the resulting estimates with a causal interpretation. A systematic approach for the integration of causal modeling with statistical estimation is presented. We highlight some common points of confusion that occur when causal modeling techniques are applied in practice and provide a broad overview on the types of questions that a causal framework can help to address. Our aims are to argue for the utility of formal causal thinking, to clarify what causal models can and cannot do, and to provide an accessible introduction to the flexible and powerful tools provided by causal models.},
	language = {eng},
	number = {3},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Petersen, Maya L. and van der Laan, Mark J.},
	month = may,
	year = {2014},
	pmid = {24713881},
	pmcid = {PMC4077670},
	keywords = {Causality, Confounding Factors, Epidemiologic, Epidemiologic Methods, Humans, Models, Statistical, Sensitivity and Specificity},
	pages = {418--426},
	file = {Accepted Version:/home/imbroglio/Zotero/storage/BRIFEDS2/Petersen and van der Laan - 2014 - Causal models and learning from data integrating .pdf:application/pdf},
}

@article{andersen_competing_2012,
	title = {Competing risks in epidemiology: possibilities and pitfalls},
	volume = {41},
	issn = {0300-5771},
	shorttitle = {Competing risks in epidemiology},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3396320/},
	doi = {10.1093/ije/dyr213},
	abstract = {Background In studies of all-cause mortality, the fundamental epidemiological concepts of rate and risk are connected through a well-defined one-to-one relation. An important consequence of this relation is that regression models such as the proportional hazards model that are defined through the hazard (the rate) immediately dictate how the covariates relate to the survival function (the risk)., Methods This introductory paper reviews the concepts of rate and risk and their one-to-one relation in all-cause mortality studies and introduces the analogous concepts of rate and risk in the context of competing risks, the cause-specific hazard and the cause-specific cumulative incidence function., Results The key feature of competing risks is that the one-to-one correspondence between cause-specific hazard and cumulative incidence, between rate and risk, is lost. This fact has two important implications. First, the naïve Kaplan–Meier that takes the competing events as censored observations, is biased. Secondly, the way in which covariates are associated with the cause-specific hazards may not coincide with the way these covariates are associated with the cumulative incidence. An example with relapse and non-relapse mortality as competing risks in a stem cell transplantation study is used for illustration., Conclusion The two implications of the loss of one-to-one correspondence between cause-specific hazard and cumulative incidence should be kept in mind when deciding on how to make inference in a competing risks situation.},
	number = {3},
	urldate = {2020-08-11},
	journal = {International Journal of Epidemiology},
	author = {Andersen, Per Kragh and Geskus, Ronald B and de Witte, Theo and Putter, Hein},
	month = jun,
	year = {2012},
	pmid = {22253319},
	pmcid = {PMC3396320},
	pages = {861--870},
	file = {PubMed Central Full Text PDF:/home/imbroglio/Zotero/storage/7TK6PATZ/Andersen et al. - 2012 - Competing risks in epidemiology possibilities and.pdf:application/pdf},
}

@article{rytgaard_continuous-time_2021,
	title = {Continuous-time targeted minimum loss-based estimation of intervention-specific mean outcomes},
	url = {http://arxiv.org/abs/2105.02088},
	abstract = {This paper studies the generalization of the targeted minimum loss-based estimation (TMLE) framework to estimation of effects of time-varying interventions in settings where both interventions, covariates, and outcome can happen at subject-specific time-points on an arbitrarily fine time-scale. TMLE is a general template for constructing asymptotically linear substitution estimators for smooth low-dimensional parameters in infinite-dimensional models. Existing longitudinal TMLE methods are developed for data where observations are made on a discrete time-grid. We consider a continuous-time counting process model where intensity measures track the monitoring of subjects, and focus on a low-dimensional target parameter defined as the intervention-specific mean outcome at the end of follow-up. To construct our TMLE algorithm for the given statistical estimation problem we derive an expression for the efficient influence curve and represent the target parameter as a functional of intensities and conditional expectations. The high-dimensional nuisance parameters of our model are estimated and updated in an iterative manner according to separate targeting steps for the involved intensities and conditional expectations. The resulting estimator solves the efficient influence curve equation. We state a general efficiency theorem and describe a highly adaptive lasso estimator for nuisance parameters that allows us to establish asymptotic linearity and efficiency of our estimator under minimal conditions on the underlying statistical model.},
	urldate = {2021-08-24},
	journal = {arXiv:2105.02088 [math, stat]},
	author = {Rytgaard, Helene C. and Gerds, Thomas A. and van der Laan, Mark J.},
	month = may,
	year = {2021},
	note = {arXiv: 2105.02088},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
	annote = {Comment: 27 pages (excluding supplementary material), 1 figures},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/CBANU96G/Rytgaard et al. - 2021 - Continuous-time targeted minimum loss-based estima.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/IDR6CKI9/2105.html:text/html},
}

@article{rytgaard_estimation_2021,
	title = {Estimation of {Time}-{Specific} {Intervention} {Effects} on {Continuously} {Distributed} {Time}-to-{Event} {Outcomes} by {Targeted} {Maximum} {Likelihood} {Estimation}},
	url = {http://arxiv.org/abs/2106.11009},
	abstract = {Targeted maximum likelihood estimation is a general methodology combining flexible ensemble learning and semiparametric efficiency theory in a two-step procedure for estimation of causal parameters. Proposed targeted maximum likelihood procedures for survival and competing risks analysis have so far focused on events taken values in discrete time. We here present a targeted maximum likelihood estimation procedure for event times that take values in R+. We focuson the estimation of intervention-specific mean outcomes with stochastic interventions on a time-fixed treatment. For data-adaptive estimation of nuisance parameters, we propose a new flexible highly adaptive lasso estimation method for continuous-time intensities that can be implemented with L1-penalized Poisson regression. In a simulation study the targeted maximum likelihood estimator based on the highly adaptive lasso estimator proves to be unbiased and achieve proper coverage in agreement with the asymptotic theory and further displays efficiency improvements relative to a Kaplan-Meier approach.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.11009 [stat]},
	author = {Rytgaard, Helene Charlotte Wiese and Eriksson, Frank and van der Laan, Mark},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.11009},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/QKPRZ8IV/Rytgaard et al. - 2021 - Estimation of time-specific intervention effects o.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/CKBFIC3X/2106.html:text/html},
}

@article{austin_accounting_2017,
	title = {Accounting for competing risks in randomized controlled trials: a review and recommendations for improvement},
	volume = {36},
	issn = {1097-0258},
	shorttitle = {Accounting for competing risks in randomized controlled trials},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7215},
	doi = {10.1002/sim.7215},
	abstract = {In studies with survival or time-to-event outcomes, a competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. Specialized statistical methods must be used to analyze survival data in the presence of competing risks. We conducted a review of randomized controlled trials with survival outcomes that were published in high-impact general medical journals. Of 40 studies that we identified, 31 (77.5\%) were potentially susceptible to competing risks. However, in the majority of these studies, the potential presence of competing risks was not accounted for in the statistical analyses that were described. Of the 31 studies potentially susceptible to competing risks, 24 (77.4\%) reported the results of a Kaplan–Meier survival analysis, while only five (16.1\%) reported using cumulative incidence functions to estimate the incidence of the outcome over time in the presence of competing risks. The former approach will tend to result in an overestimate of the incidence of the outcome over time, while the latter approach will result in unbiased estimation of the incidence of the primary outcome over time. We provide recommendations on the analysis and reporting of randomized controlled trials with survival outcomes in the presence of competing risks. © 2017 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
	language = {en},
	number = {8},
	urldate = {2021-08-25},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Fine, Jason P.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7215},
	keywords = {survival analysis, competing risks, randomized controlled trial, RCT, systematic review},
	pages = {1203--1209},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/CE3LR5KC/Austin and Fine - 2017 - Accounting for competing risks in randomized contr.pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/EY7P76X9/sim.html:text/html},
}

@article{austin_practical_2017,
	title = {Practical recommendations for reporting {Fine}-{Gray} model analyses for competing risk data},
	volume = {36},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7501},
	doi = {10.1002/sim.7501},
	abstract = {In survival analysis, a competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. Outcomes in medical research are frequently subject to competing risks. In survival analysis, there are 2 key questions that can be addressed using competing risk regression models: first, which covariates affect the rate at which events occur, and second, which covariates affect the probability of an event occurring over time. The cause-specific hazard model estimates the effect of covariates on the rate at which events occur in subjects who are currently event-free. Subdistribution hazard ratios obtained from the Fine-Gray model describe the relative effect of covariates on the subdistribution hazard function. Hence, the covariates in this model can also be interpreted as having an effect on the cumulative incidence function or on the probability of events occurring over time. We conducted a review of the use and interpretation of the Fine-Gray subdistribution hazard model in articles published in the medical literature in 2015. We found that many authors provided an unclear or incorrect interpretation of the regression coefficients associated with this model. An incorrect and inconsistent interpretation of regression coefficients may lead to confusion when comparing results across different studies. Furthermore, an incorrect interpretation of estimated regression coefficients can result in an incorrect understanding about the magnitude of the association between exposure and the incidence of the outcome. The objective of this article is to clarify how these regression coefficients should be reported and to propose suggestions for interpreting these coefficients.},
	language = {en},
	number = {27},
	urldate = {2021-08-25},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Fine, Jason P.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7501},
	keywords = {survival analysis, competing risks, cumulative incidence function, subdistribution hazard model},
	pages = {4391--4400},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/PGUF7YRC/Austin and Fine - 2017 - Practical recommendations for reporting Fine-Gray .pdf:application/pdf;Snapshot:/home/imbroglio/Zotero/storage/P2Y77HPN/sim.html:text/html},
}

@article{rytgaard_one-step_2021,
	title = {One-step {TMLE} for targeting cause-specific absolute risks and survival curves},
	url = {http://arxiv.org/abs/2107.01537},
	abstract = {This paper considers one-step targeted maximum likelihood estimation method for general competing risks and survival analysis settings where event times take place on the positive real line R+ and are subject to right-censoring. Our interest is overall in the effects of baseline treatment decisions, static, dynamic or stochastic, possibly confounded by pre-treatment covariates. We point out two overall contributions of our work. First, our method can be used to obtain simultaneous inference across all absolute risks in competing risks settings. Second, we present a practical result for achieving inference for the full survival curve, or a full absolute risk curve, across time by targeting over a fine enough grid of points. The one-step procedure is based on a one-dimensional universal least favorable submodel for each cause-specific hazard that can be implemented in recursive steps along a corresponding universal least favorable submodel. We present a theorem for conditions to achieve weak convergence of the estimator for an infinite-dimensional target parameter. Our empirical study demonstrates the use of the methods.},
	urldate = {2022-04-20},
	journal = {arXiv:2107.01537 [stat]},
	author = {Rytgaard, Helene C. W. and van der Laan, Mark J.},
	month = sep,
	year = {2021},
	note = {arXiv: 2107.01537
version: 2},
	keywords = {Statistics - Methodology},
	annote = {Comment: 21 pages (including appendix), 1 figure, 5 tables},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/GYPIFDDQ/Rytgaard and van der Laan - 2021 - One-step TMLE for targeting cause-specific absolut.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/V455MJ7G/2107.html:text/html},
}

@misc{cui_estimating_2022,
	title = {Estimating heterogeneous treatment effects with right-censored data via causal survival forests},
	url = {http://arxiv.org/abs/2001.09887},
	doi = {10.48550/arXiv.2001.09887},
	abstract = {Forest-based methods have recently gained in popularity for non-parametric treatment effect estimation. Building on this line of work, we introduce causal survival forests, which can be used to estimate heterogeneous treatment effects in a survival and observational setting where outcomes may be right-censored. Our approach relies on orthogonal estimating equations to robustly adjust for both censoring and selection effects under unconfoundedness. In our experiments, we find our approach to perform well relative to a number of baselines.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Cui, Yifan and Kosorok, Michael R. and Sverdrup, Erik and Wager, Stefan and Zhu, Ruoqing},
	month = sep,
	year = {2022},
	note = {Number: arXiv:2001.09887
arXiv:2001.09887 [cs, stat]},
	keywords = {Statistics - Methodology, 62N01, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/2FWR4EPB/Cui et al. - 2022 - Estimating heterogeneous treatment effects with ri.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/2Y6EEQTP/2001.html:text/html},
}

@misc{westling_inference_2021,
	title = {Inference for treatment-specific survival curves using machine learning},
	url = {http://arxiv.org/abs/2106.06602},
	abstract = {In the absence of data from a randomized trial, researchers often aim to use observational data to draw causal inference about the effect of a treatment on a time-to-event outcome. In this context, interest often focuses on the treatment-specific survival curves; that is, the survival curves were the entire population under study to be assigned to receive the treatment or not. Under certain causal conditions, including that all confounders of the treatment-outcome relationship are observed, the treatment-specific survival can be identified with a covariate-adjusted survival function. Several estimators of this function have been proposed, including estimators based on outcome regression, inverse probability weighting, and doubly robust estimators. In this article, we propose a new cross-fitted doubly-robust estimator that incorporates data-adaptive (e.g. machine learning) estimators of the conditional survival functions. We establish conditions on the nuisance estimators under which our estimator is consistent and asymptotically linear, both pointwise and uniformly in time. We also propose a novel ensemble learner for combining multiple candidate estimators of the conditional survival estimators. Notably, our methods and results accommodate events occurring in discrete or continuous time (or both). We investigate the practical performance of our methods using numerical studies and an application to the effect of a surgical treatment to prevent metastases of parotid carcinoma on mortality.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Westling, Ted and Luedtke, Alex and Gilbert, Peter and Carone, Marco},
	month = jun,
	year = {2021},
	note = {Number: arXiv:2106.06602
arXiv:2106.06602 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/KV7I36NK/Westling et al. - 2021 - Inference for treatment-specific survival curves u.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/SXQSG6BH/2106.html:text/html},
}

@misc{denz_comparison_2022,
	title = {A {Comparison} of {Different} {Methods} to {Adjust} {Survival} {Curves} for {Confounders}},
	url = {http://arxiv.org/abs/2203.10002},
	doi = {10.48550/arXiv.2203.10002},
	abstract = {Treatment specific survival curves are an important tool to illustrate the treatment effect in studies with time-to-event outcomes. In non-randomized studies, unadjusted estimates can lead to biased depictions due to confounding. Multiple methods to adjust survival curves for confounders exist. However, it is currently unclear which method is the most appropriate in which situation. Our goal is to compare these methods in different scenarios with a focus on their bias and goodness-of-fit. We provide a short review of all methods and illustrate their usage by contrasting the survival of smokers and non-smokers, using data from the German Epidemiological Trial on Ankle Brachial Index. Subsequently, we compare the methods using a Monte-Carlo simulation. We consider scenarios in which correctly or incorrectly specified covariate sets for describing the treatment assignment and the time-to-event outcome are used with varying sample sizes. The bias and goodness-of-fit is determined by summary statistics which take into account the entire survival curve. When used properly, all methods showed no systematic bias in medium to large samples. Cox-Regression based methods, however, showed systematic bias in small samples. The goodness-of-fit varied greatly between different methods and scenarios. Methods utilizing an outcome model were more efficient than other techniques, while augmented estimators using an additional treatment assignment model were unbiased when either model was correct with a goodness-of-fit comparable to other methods. These "doubly-robust" methods have important advantages in every considered scenario. Pseudo-Value based methods, coupled with isotonic regression to correct for non-monotonicity, are viable alternatives to traditional methods.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Denz, Robin and Klaaßen-Mielke, Renate and Timmesfeld, Nina},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.10002
arXiv:2203.10002 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 26 pages, 5 figures, submitted to "Statistics in Medicine" as research article, accepted for oral presentation at the International Biometric Conference 2022},
	file = {arXiv Fulltext PDF:/home/imbroglio/Zotero/storage/9JW5FYKM/Denz et al. - 2022 - A Comparison of Different Methods to Adjust Surviv.pdf:application/pdf;arXiv.org Snapshot:/home/imbroglio/Zotero/storage/E8FZL9TQ/2203.html:text/html},
}

@misc{sofrygin_stremr_2017,
	title = {stremr: {Streamlined} {Estimation} of {Survival} for {Static}, {Dynamic} and {Stochastic} {Treatment} and {Monitoring} {Regimes}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {stremr},
	url = {https://CRAN.R-project.org/package=stremr},
	abstract = {Analysis of longitudinal time-to-event or time-to-failure data. Estimates the counterfactual discrete survival curve under static, dynamic and stochastic interventions on treatment (exposure) and monitoring events over time. Estimators (IPW, MSM-IPW, GCOMP, longitudinal TMLE) adjust for measured time-varying confounding and informative right-censoring. Model fitting can be performed either with GLM or H2O-3 machine learning libraries. The exposure, monitoring and censoring variables can be coded as either binary, categorical or continuous. Each can be multivariate (e.g., can use more than one column of dummy indicators for different censoring events). The input data needs to be in long format.},
	urldate = {2022-09-25},
	author = {Sofrygin, Oleg and Laan, Mark J. van der and Neugebauer, Romain},
	month = jan,
	year = {2017},
}

@misc{schwab_ltmle_2020,
	title = {ltmle: {Longitudinal} {Targeted} {Maximum} {Likelihood} {Estimation}},
	copyright = {GPL-2},
	shorttitle = {ltmle},
	url = {https://CRAN.R-project.org/package=ltmle},
	abstract = {Targeted Maximum Likelihood Estimation (TMLE) of treatment/censoring specific mean outcome or marginal structural model for point-treatment and longitudinal data.},
	urldate = {2022-09-25},
	author = {Schwab, Joshua and Lendle, Samuel and Petersen, Maya and Laan, Mark van der and Gruber, Susan},
	month = mar,
	year = {2020},
	keywords = {CausalInference},
}

@misc{benkeser_survtmle_2019,
	title = {survtmle: {Compute} {Targeted} {Minimum} {Loss}-{Based} {Estimates} in {Right}-{Censored} {Survival} {Settings}},
	copyright = {MIT + file LICENSE},
	shorttitle = {survtmle},
	url = {https://CRAN.R-project.org/package=survtmle},
	abstract = {Targeted estimates of marginal cumulative incidence in survival settings with and without competing risks, including estimators that respect bounds (Benkeser, Carone, and Gilbert. Statistics in Medicine, 2017. {\textless}doi:10.1002/sim.7337{\textgreater}).},
	urldate = {2022-09-25},
	author = {Benkeser, David and Hejazi, Nima},
	month = apr,
	year = {2019},
}

@misc{polley_superlearner_2021,
	title = {{SuperLearner}: {Super} {Learner} {Prediction}},
	copyright = {GPL-3},
	shorttitle = {{SuperLearner}},
	url = {https://CRAN.R-project.org/package=SuperLearner},
	abstract = {Implements the super learner prediction method and contains a library of prediction algorithms to be used in the super learner.},
	urldate = {2022-09-25},
	author = {Polley, Eric and LeDell, Erin and Kennedy, Chris and Lendle, Sam and Laan, Mark van der},
	month = may,
	year = {2021},
	keywords = {MachineLearning},
}

@misc{gerds_riskregression_2022,
	title = {{riskRegression}: {Risk} {Regression} {Models} and {Prediction} {Scores} for {Survival} {Analysis} with {Competing} {Risks}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {{riskRegression}},
	url = {https://CRAN.R-project.org/package=riskRegression},
	abstract = {Implementation of the following methods for event history analysis. Risk regression models for survival endpoints also in the presence of competing risks are fitted using binomial regression based on a time sequence of binary event status variables. A formula interface for the Fine-Gray regression model and an interface for the combination of cause-specific Cox regression models. A toolbox for assessing and comparing performance of risk predictions (risk markers and risk prediction models). Prediction performance is measured by the Brier score and the area under the ROC curve for binary possibly time-dependent outcome. Inverse probability of censoring weighting and pseudo values are used to deal with right censored data. Lists of risk markers and lists of risk models are assessed simultaneously. Cross-validation repeatedly splits the data, trains the risk prediction models on one part of each split and then summarizes and compares the performance across splits.},
	urldate = {2022-09-25},
	author = {Gerds, Thomas Alexander and Ohlendorff, Johan Sebastian and Blanche, Paul and Mortensen, Rikke and Wright, Marvin and Tollenaar, Nikolaj and Muschelli, John and Mogensen, Ulla Brasch and Ozenne, Brice},
	month = sep,
	year = {2022},
	keywords = {CausalInference, Survival},
}

@misc{wallace_dtrreg_2020,
	title = {{DTRreg}: {DTR} {Estimation} and {Inference} via {G}-{Estimation}, {Dynamic} {WOLS}, {Q}-{Learning}, and {Dynamic} {Weighted} {Survival} {Modeling} ({DWSurv})},
	copyright = {GPL-2},
	shorttitle = {{DTRreg}},
	url = {https://CRAN.R-project.org/package=DTRreg},
	abstract = {Dynamic treatment regime estimation and inference via G-estimation, dynamic weighted ordinary least squares (dWOLS) and Q-learning. Inference via bootstrap and (for G-estimation) recursive sandwich estimation. Estimation and inference for survival outcomes via Dynamic Weighted Survival Modeling (DWSurv). Extension to continuous treatment variables (gdwols). Wallace et al. (2017) {\textless}doi:10.18637/jss.v080.i02{\textgreater}; Simoneau et al. (2020) {\textless}doi:10.1080/00949655.2020.1793341{\textgreater}.},
	urldate = {2022-09-25},
	author = {Wallace, Michael and Moodie, Erica E. M. and Stephens, David A. and Schulz, Gabrielle Simoneau {and} Juliana},
	month = sep,
	year = {2020},
	keywords = {CausalInference},
}

@article{laan_statistical_2006,
	title = {Statistical {Inference} for {Variable} {Importance}},
	volume = {2},
	issn = {1557-4679},
	url = {https://www.degruyter.com/document/doi/10.2202/1557-4679.1008/html?lang=en},
	doi = {10.2202/1557-4679.1008},
	abstract = {Many statistical problems involve the learning of an importance/effect of a variable for predicting an outcome of interest based on observing a sample of \$n\$ independent and identically distributed observations on a list of input variables and an outcome. For example, though prediction/machine learning is, in principle, concerned with learning the optimal unknown mapping from input variables to an outcome from the data, the typical reported output is a list of importance measures for each input variable. The approach in prediction has been to learn the unknown optimal predictor from the data and derive, for each of the input variables, the variable importance from the obtained fit. In this article we propose a new approach which involves for each variable separately 1) defining variable importance as a real valued parameter, 2) deriving the efficient influence curve and thereby optimal estimating function for this parameter in the assumed (possibly nonparametric) model, and 3) develop a corresponding double robust locally efficient estimator of this variable importance, obtained by substituting for the nuisance parameters in the optimal estimating function data adaptive estimators. We illustrate this methodology in the context of prediction, and obtain in this manner double robust locally optimal estimators of marginal variable importance, accompanied with p-values and confidence intervals. In addition, we present a model based and machine learning approach to estimate covariate-adjusted variable importance. Finally, we generalize this methodology to variable importance parameters for time-dependent variables.},
	language = {en},
	number = {1},
	urldate = {2022-09-25},
	journal = {The International Journal of Biostatistics},
	author = {Laan, Mark J. van der},
	month = feb,
	year = {2006},
	note = {Publisher: De Gruyter},
	keywords = {adjusted-variable importance, causal effect, efficient influence curve, estimating function, prediction, variable importance},
	file = {Full Text PDF:/home/imbroglio/Zotero/storage/XC22XPE6/Laan - 2006 - Statistical Inference for Variable Importance.pdf:application/pdf},
}
@incollection{kennedy2016semiparametric,
  title={Semiparametric theory and empirical processes in causal inference},
  author={Kennedy, Edward H},
  booktitle={Statistical causal inferences and their applications in public health research},
  pages={141--167},
  year={2016},
  publisher={Springer}
}
