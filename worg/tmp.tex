% Created 2022-09-27 Tue 20:27
% Intended LaTeX compiler: pdflatex
\documentclass{report}
                              \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{RJournal}
\usepackage{amsmath,amssymb,array}
\usepackage{booktabs}
\sectionhead{Contributed research article}
\volume{XX}
\volnumber{ZZ}
\year{20YY}
\month{MM}
\usepackage{blindtext}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan}
\usepackage{float}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\J}{\ensuremath{J}}
\newcommand{\1}{\ensuremath{\mathbf{1}}}
\newcommand{\h}{\ensuremath{\lambda}}
\newcommand{\indep}{\ensuremath{\perp\hspace*{-1.4ex}\perp}}
\newcommand{\T}{\ensuremath{\widetilde{T}}}
\newcommand{\X}{\ensuremath{{W}}}
\renewcommand{\t}{\ensuremath{\Tilde{t}}}
\newcommand{\ax}{\ensuremath{\mid a,\,{w}}}
\newcommand{\aX}{\ensuremath{\mid A = a,\,{W}}}
\newcommand{\AX}{\ensuremath{\mid A,\,{W}}}
\newcommand{\x}{\ensuremath{{w}}}
\newcommand{\trt}{\ensuremath{\pi^*}}
\newcommand{\tk}{\ensuremath{\tau}}
\newcommand{\lj}{\ensuremath{l}}
\newcommand{\jj}{\ensuremath{j}}
\newcommand{\tK}{\ensuremath{K}}
\newcommand{\tKi}{\ensuremath{k}}
\newcommand{\TK}{\ensuremath{\mathcal{T}}}
\newcommand{\g}{\ensuremath{\pi}}
\renewcommand{\L}{\ensuremath{W}}
\renewcommand{\l}{\ensuremath{w}}
\setcounter{secnumdepth}{5}

\lstset{
keywordstyle=\color{blue},
commentstyle=\color{red},stringstyle=\color[rgb]{0,.5,0},
literate={~}{$\sim$}{1},
basicstyle=\ttfamily\small,
columns=fullflexible,
breaklines=true,
breakatwhitespace=false,
numbers=left,
numberstyle=\ttfamily\tiny\color{gray},
stepnumber=1,
numbersep=10pt,
backgroundcolor=\color{white},
tabsize=4,
keepspaces=true,
showspaces=false,
showstringspaces=false,
xleftmargin=.23in,
frame=single,
basewidth={0.5em,0.4em},
}
\renewcommand*\familydefault{\sfdefault}
\itemsep2pt
\author{imbroglio}
\date{\today}
\title{}
\begin{document}

\title{concrete R Paper}
\subtitle{}
\author{by David Chen, Thomas Gerds, Helene Rytgaard, Maya L. Petersen, Mark van der Laan, ...}

\maketitle

\section{Introduction}
\label{intro}
\subsection{what concrete is}
\label{sec:orgfa58f06}

The R package \CRANpkg{concrete} answers causal questions with time-to-event outcomes in continous time. It implements the TMLE method developed in \cite{rytgaard_one-step_2021} to estimate time-point specific average treatment effects and returns absolute t-year risks as well as differences and ratios of absolute \(t\)-year risks with asymptotic inference based on the efficient influence curve (\cite{laan_unified_2003-1}).

\subsection{what is in this manuscript}
\label{sec:org6837d6d}

We write for readers looking for a hands-on introduction to the one-step TMLE method for
continuous time survival analysis described in \cite{rytgaard_one-step_2021} as well as to readers
who want to use our package for their own analyses.

\subsection{what concrete does and does not do}
\label{sec:orgea90b1b}
The package can be used for targeted estimation of estimands derived from cause-specific absolute risks (e.g. relative risks and risk differences) under static and dynamic binary treatments given at baseline. It deals with baseline covariate confounding, right censoring and competing risks.

The package cannot (yet) analyse multiple treatments, continuous or multinomial treatments, or stochastic interventions. Methods to account for paired or clustered data is not yet supported, nor is support for time-dependent treatments (e.g. drop-in) and time-dependent confounding.

This package is not meant to be used for left trunctation (i.e. delayed entry) or interval censored data, 

\subsection{how it relates to other peoples work}
\label{sec:org2ed371d}

The \CRANpkg{ltmle} (\cite{schwab_ltmle_2020}), \CRANpkg{stremr} (\cite{sofrygin_stremr_2017}), and \CRANpkg{survtmle} (\cite{benkeser_survtmle_2019}) TMLE implementations either natively or can be adapted to estimate absolute risks of right-censored survival outcomes; \CRANpkg{ltmle} and \CRANpkg{stremr} use the method of iterated expectations while \CRANpkg{survtmle} can target the hazard-based survival formulation. Notably, these packages all operate in discrete time and thus requires discretization of continuous-time data. Poorly specified discretization can introduce bias and inflate the variance of estimators and no definitive best practices have yet been established, leading to ad-hoc choices that make unknown trade offs between bias and loss of efficiency. Analyzing continuous-time survival data using a continuous-time method avoids this hurdle entirely; \CRANpkg{concrete} will be the first R package implementing TMLE for continuous-time survival.

The \ctv{Causal Inference} CRAN Task View shows only \CRANpkg{riskregression} (\cite{gerds_riskregression_2022}) and \CRANpkg{DTRreg} (\cite{wallace_dtrreg_2020}) as expressly implementing estimation of survival estimands; both packages do so using g-formula plug-in estimators. The \ctv{Survival} CRAN Task View does not show any packages as expressly implementing efficient semi-parametric estimators for survival estimands. Not on CRAN are the packages \href{https://github.com/RobinDenz1/adjustedCurves}{adjustedCurves} (\cite{denz_comparison_2022}) (iptw, g-formula, aiptw \& others) and \href{https://github.com/tedwestling/CFsurvival}{CFsurvival} (\cite{westling_inference_2021}) (one-step estimator). 


\section{Concepts}
\label{sec:orga6cf914}

\subsection{The Targeted Learning Roadmap}
\label{sec:org89b1abd}
When we analyze survival data and make predictions to guide future actions, we are in essence asking causal questions. The formal causal inference frameworks developed in recent decades allow us to make this process rigorous; a systematic roadmap \cite{petersen_causal_2014} integrates causal modeling with statistical estimation into a cohesive causal inference workflow.

In the continuous-time survival case, the targeted learning roadmap consists of:
\begin{enumerate}
  \item causal considerations including defining a time zero and time horizon, specifying the event(s) of interest, identifying the intervention variable and specifying the desired interventions, and defining causal estimand(s) (e.g. evaluation time(s), difference vs. ratio)
  \item statistical considerations such as identifying confounding variables, identifying right-censoring, establishing positivity for desired interventions and remaining uncensored, formalizing knowledge about the statistical model (e.g. dependency/independence structure, proportional hazards, etc.)
  \item estimation using a pre-specified estimator
\end{enumerate}

\subsection{The Causal Model: Counterfactuals, Interventions, and Causal Estimands}
\label{sec:org6810eb5}
Counterfactual survival data with \(J\) competing events takes the form:
\begin{equation}
 X = \left(\T^d,\, \Delta^d,\, \L \right) \label{causaldata}
\end{equation}
where the superscript \(d\) indicates desired intervention, \(\T^d \in \mathbb{R}^+\) is earliest occurence of any event under that intervention, \(\Delta^d \in \{1, \dots, J\}\) indicates which event occurs first under that intervention, and \(\L\) is a set of covariates measured at baseline. 

Equation \eqref{causalrisk} is the causal event-\(j\) absolute risk at time \(t\) under intervention \(d\) and Equation \eqref{causalsurv} is the causal event-free survival at time \(t\) under intervention \(d\).

\begin{equation}
 P(T^d_j \leq t, \Delta^d = j) \label{causalrisk}
\end{equation}
\begin{equation}
 P(T^d_j < t) \label{causalsurv}
\end{equation}

\subsection{Observed Data}
\label{ObservedData}
Observed survival data with \(J\) competing events takes the form:
\begin{equation}
 O = \left(\T,\, \Delta,\, A,\, \L \right) \label{obs-data}
\end{equation}
where \(A\) is the intervention variable, \(\T \in \mathbb{R}^+\) is the earlier of the time to first observed event or the onset of right-censoring, \(\Delta \in \{0, \dots, J\}\) indicates which event occurs (with 0 indicating right-censoring), and \(\L\) is a set of covariates measured at baseline.

Given the identification assumptions of consistency, positivity for treatments and remaining uncensored, no unmeasured confounding, and coarsening at random on the censoring process, the causal survival \eqref{causalsurv} and risk \eqref{causalrisk} estimands can be identified by statistical estimands which are purely functions of the observed data. If the identification assumptions (stated in detail in Appendix \ref{identification}) do not hold, then these statistical estimands still have an interpretation as standardized risks isolating the importance of the "intervention" variable (\cite{laan_statistical_2006}).  

\subsection{Estimation}
\label{estimation}
\CRANpkg{concrete} implements the one-step TMLE for right-censored survival and competing risks in continuous time described by \cite{rytgaard_continuous-time_2021} and \cite{rytgaard_one-step_2021} which consists of two major steps: an initial estimation of nuisance parameters utilizing flexible machine learning and a subsequent targeted update of initial estimators to solve the efficient influence function of the target statistical estimand (\cite{laan_unified_2003-1,kennedy2016semiparametric}).

Adequate convergence is needed in the initial estimation stage so \CRANpkg{concrete} uses flexible machine learning ensemble with oracle guarantees (\cite{laan_super_2007}, \cite{polley_superlearner_2021}, \cite{laan_unified_2003}, \cite{vaart_oracle_2006}), particularly with a candidate library incorporating the highly adaptive lasso (HAL) as it converges at the required rate (\cite{laan_generally_2017,benkeser_highly_2016,rytgaard_continuous-time_2021}). 

The subsequent update or targeting step leans on a result from semi-parametric efficiency theory (\cite{bickel_efficient_1998}), which states that a regular, asymptotically linear estimator for a statistical target parameter in a semiparametric model is asymptotically efficient if its influence function is equal to the efficient influence curve (EIC). The efficient influence function for the cause-specific absolute risk at every target time \tk for every desired treatment regime \(\trt\) and every target event \jj is: 

\begin{align}
    D^*_{\trt, \jj, \tk}(\Hat \lambda, \Hat \g, \Hat S_c)(O) = \sum_{\lj = 1}^{J} \sum_{\tKi = 1}^{\tK} \; &h_{\trt,\, \jj,\, \lj,\, \tk, s}(\Hat \lambda, \Hat \g, \Hat S_c)(O) \, \left(\1(\Delta = \jj, \T = s_{\tKi}) - \1(\T \geq s_\tK) \, \Hat \lambda_\lj(s_{\tKi} \AX)\right) \label{eic} \\[-4mm]
    &{\color{blue!60!black}+ \sum_{a\,\in\,\mathcal{A}} F_\jj(\tk \mid A = a, \X)\,\trt(a \mid \X) - \Psi_{\trt, \jj, \tk}(P_0)}  \nonumber 
\end{align}
where 
\begin{align}
    h_{\trt,\, \jj,\, \lj,\, \tk,\, s}&(\Hat \lambda, \Hat \g, \Hat S_c)(O) = \frac{{\color{blue}\trt(A \mid \X)\,} \1(s \leq \tk)}{{\color{green!70!black}\Hat\g^{SL}(A \mid \X) \;
\Hat S_c(s\text{-} \AX)}} \, \bigg(\1(\Delta = \jj) - \frac{{\color{red}\Hat F_\jj(\tk \AX)} - {\color{red}\Hat F_\jj(s \AX)}}{{\color{red}\Hat S(s \AX)}}\bigg) \label{clevcov}
\end{align}

The clever covariate is a function of the {\color{blue}desired intervention density} which is user specified, the {\color{green!70!black} observed intervention densities} which are not changed by tmle targeting, and the {\color{red}outcome-related densities} which are updated by targeting.

The one-step continuous-time survival TMLE updates the cause-specific hazards in small steps along the sequence of locally-least favorable submodels in the following manner:
\begin{equation}
\Hat \lambda_{j, \epsilon_m}(t) = \Hat\lambda^{SL}_{j}(t) \, \exp\left(\sum_{i = 1}^{m}\frac{\left<\mathbb{P}_n D^*(\Hat \lambda_{\epsilon_i}, \Hat \g, \Hat S_c)(O),\; h_{j, s}(\Hat \lambda_{\epsilon_i}, \Hat \g, \Hat S_c)(O) \right>_{\Sigma}}{|| D^*(\Hat \lambda_{\epsilon_i}, \Hat \g, \Hat S_c)(O)||_{\Sigma}} \; \epsilon_i \right) \label{onestep}
\end{equation}
where
\[ \left<x, y\right>_{\Sigma} = x^\top \Sigma^{\text{ -}1} y \hspace{.5cm}, \hspace{.5cm} ||x||_{\Sigma} = \sqrt{x^\top \Sigma^{\text{ -}1} x} \]
\begin{align*}
\intertext{\(D^*\) is the vector of efficient influence functions}
D^{*}(\lambda, \g, S_c)(O) &= \left(D^*_{\trt, \jj, \tk}(\lambda, \g, S_c)(O) : \trt \in \mathcal{A}, \jj \in \mathcal{J}, \tk \in \TK)\right)
\intertext{and \(h_{j, s}\) is the vector of clever covariates}
h_{j, s}(\lambda, \g, S_c)(O) &= \left(h_{\trt, \jj, \lj, \tk, s}(\lambda, \g, S_c)(O) : \trt \in \mathcal{A}, \jj \in \mathcal{J}, \tk \in \TK)\right)
\end{align*}


The one-step TMLE algorithm stops at the \(\epsilon_i\) when
\begin{equation}
\mathbb{P}_n D^*(\Hat \lambda_{\epsilon_i}, \Hat \g, \Hat S_c)(O) \leq \frac{\sqrt{\mathbb{P}_n \;D^*(\Hat \lambda_{\epsilon_i}, \Hat \g, \Hat S_c)(O)^2}}{\sqrt{n} \, \log(n)} \label{one-step-stop}
\end{equation}


\section{Usage}
\label{UsingConcrete}
\CRANpkg{concrete} was written for causal analyses of time-to-event data, though it can also be used for purely statistical estimation problems. There are 3 main user-facing functions in \CRANpkg{concrete}: \code{formatArguments()}, \code{doConcrete}, and \code{getOutput}. Reflecting our vision of good statistical practice, the majority of user effort is directed into defining the desired analysis through specifying arguments into \code{formatArguments()}. The output of \code{formatArguments()} is a \code{"ConcreteArgs"} object which is passed into \code{doConcrete()} to perform the specified continuous-time one-step survival TMLE. The output of \code{doConcrete()} is a \code{"ConcreteEst"} object which can be passed into getOutput to print, summarize, and plot cause-specific absolute risk derived estimands such as risk differences and relative risks.

\subsection{formatArguments()}
\label{formatArguments}
Arguments into \code{formatArguments} fall into 3 broad categories: specifying the observed data structure, specifying the target estimand, and specifying the estimation algorithm. \code{formatArguments()} will check its inputs and return errors, warnings, and messages as necessary. The output of \code{formatArguments} is an object of class \code{"ConcreteArgs"}, with fields that can be modified by the user before passing the \code{"ConcreteArgs"} object back through \code{formatArguments} to be re-checked. This process can be repeated as necessary until the full estimation problem is adequately specified. 

\subsubsection{Data}
\label{ObservedDataConcrete}
The general form of observed right-censored survival data, potentially with competing events, is
\[ O = \left( \T ,\; \Delta ,\; A ,\; \L  \right) \]
where \(\T\) is the observed time to first event (censoring or otherwise), \(\Delta\) indicates which event occured (with \(\Delta = 0\) indicating right-censoring), \(A\) is the intervention variable, and \(\L\) is a collection of baseline covariates.

In the PBC dataset example, \(\T\) is the column \code{"time"}, \(\Delta\) is the column \code{"status"}, \(A\) is the column \code{"trt"}, and \(\L\) consists of all the columns containing patient information observed at baseline. A column containig uniquely identifying subject IDs can be passed into \CRANpkg{concrete} as well. This data is passed into \CRANpkg{concrete} as the following:
\lstset{language=r,label=pbc formatargs silent,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id",
				Intervention = 0:1)
\end{lstlisting}

\subsubsection{Target Estimand}
\label{Estimand}
\CRANpkg{concrete} implements a continuous time one-step TMLE jointly targeting the cause-specific absolute risks at certain target times under some hypothetical treatments. 

\paragraph{Treatment}
\label{TreatmentRegime}
Let \(A\) be the intervention variable and \(d\) be an intervention on the variable. If \(A\) is binary and \(d\) is a static intervention setting everyone to \(0\) or setting everyone to \(1\), then the intervention can be passed into \CRANpkg{concrete} through the argument \code{formatArguments(Intervention = } as "0" or "1" respectively. Both interventions can be specified by passing c(0, 1).

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", Intervention = 0:1)
\end{lstlisting}

More complex dynamic interventions are passed into \code{formatArguments(Intervention =)} as a list containing a pair of functions: an "intervention" function which outputs desired treatment \textbf{\textbf{assignments}} and a "g.star" function which outputs desired treatment \textbf{\textbf{probabilities}}.

\paragraph{Target Events}
\label{TargetEvent}
In the \code{pbc} dataset, there are 3 event values encoded by the \code{status} column: 0 for censored, 1 for transplant, and 2 for death. In \CRANpkg{concrete} 0 is reserved to indicate censoring, while events of interest can be encoded as any positive integer. Setting \code{formatArguments(TargetEvent = 1:2)} for the \code{pbc} dataset specifies a joint targeting of the risk of transplant and death. By default \CRANpkg{concrete} by targets all observed non-censoring events, so leaving the \code{formatArguments(TargetEvent = NULL)} would achieve the same result.

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", 
				Intervention = 0:1, TargetEvent = 1:2)
\end{lstlisting}

\paragraph{Target Time}
\label{TargetTime}
The \code{TargetTime=} argument specifies the time(s) at which estimates of the event-specific absolute risks and/or event-free survival are desired. Target times should be restricted to the time range in which failure events are observed so \code{formatArguments()} will return an error if target time is after the last observed failure event time. If no TargetTime is provided, then \CRANpkg{concrete} will target the last observed event time, though this is likely to result in a highly variable estimate if prior censoring is substantial.

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
BadTime <- unique(obs[status > 0, max(time)]) + 1
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", 
				Intervention = 0:1, TargetEvent = 1:2, TargetTime = BadTime)
\end{lstlisting}

Error in concrete:::getTargetTime(TargetTime = unique(obs[status > 0,  : 
  TargetTime must not target times after which all individuals are Censored, 4191

The \code{TargetTime} argument can either be a single number or a vector, as one-step TMLE can target cause-specific risks at multiple times simultaneously.

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", 
				Intervention = 0:1, TargetEvent = 1:2, TargetTime = 90 * (16:24))
\end{lstlisting}

\subsubsection{Estimator Specification}
\label{EstimationSpec}
The arguments involved in estimation are the cross-validation setup \code{CVArg}, the estimation algorithms \code{Model}, the software backends \code{PropScoreBackend} and \code{HazEstBackend}, and the TMLE specification choices \code{MaxUpdateIter}, \code{OneStepEps}, and \code{MinNuisance}. It should be noted here that \code{Model} is used here to conform with common usage in statistical analysis R packages, rather than to refer to a statistical or causal model as we have in the previous sections. 

\paragraph{Cross-Validation}
\label{CV}
Let \(Q_n = \{O_i\}_{i=1}^n\) be an observed sample of \(n\) i.i.d observations of \(O \sim P_0\). For \(V\text{-fold}\) cross validation, let \(B_n = \{1, ... , V\}^n\) be a random vector that assigns the \(n\) observations into \(V\) validation folds. For each \(v \in \{1, ..., V\}\) we then define training set \(Q^\mathcal{T}_v = \{O_i : B_n(i) = v\}\) with the corresponding validation set \(Q^\mathcal{V}_v = \{O_i : B_n(i) \neq v\}\).

\CRANpkg{concrete} uses \CRANpkg{origami} to specify cross-validation folds, specifically the function
\code{origami::make\_folds()}. If no input is provided to the \code{formatArguments(CVArg= )} argument, concrete will implement a simple 10-fold cross-validation scheme.

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
CVArgs <- list(n = nrow(obs), V = 10L, fold_fun = folds_vfold, cluster_ids = NULL, strata_ids = NULL)

ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", 
				Intervention = 0:1, TargetEvent = 1:2, TargetTime = 90 * (16:24), 
				CVArg = CVArgs)
\end{lstlisting}

\paragraph{Estimating Nuisance Parameters}
\label{NuisanceEstimation}
\CRANpkg{concrete} accepts estimator specifications for estimating nuisance parameters through the argument \code{formatArguments(Model= )}. Inputs into the \code{Model=} argument must be named lists with one entry for the intervention variable, and for each of the event type including censoring. The list element corresponding to intervention must be named after the variable and the list elements corresponding to each event type must be named for the numeric value of the event type ("0" for censoring). If no input is provided for the \code{Model=} argument, \code{formatArguments()} will return a correctly formatted list, \code{.[["Model"]]}, containing default estimator specifications for each nuisance parameter, which can be then edited by the user.

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", 
				Intervention = 0:1, TargetEvent = 1:2, TargetTime = 90 * (16:24), 
				CVArg = NULL, Model = NULL)
str(ConcreteArgs[["Model"]], give.attr = FALSE)
\end{lstlisting}

\paragraph{Propensity Score Estimation}
\label{PropScore}
For the true conditional distribution of \(A\) given \(\X\), \(\g_0(\cdot \mid \X)\), and \(\Hat{\g} : Q_n \to \Hat{\g}(Q_n)\), let \(L_\g\) be a loss function such that the risk \(\mathbb{E}_0\left[L_\g(\Hat{\g}, O)\right]\) is minimized when \(\Hat{\g} = \g_0\). For instance, with a binary \(A\), we may specify the negative log loss \(L_\g(\Hat{\g}, O) = \text{-}\log\left(\Hat{\g}(1 \mid \X)^A \; \Hat{\g}(0 \mid \X))^{1-A}\right)\). \CRANpkg{concrete} uses a discrete SuperLearner selector which chooses from a set of candidate models \(\mathcal{M_\g}\) the candidate propensity score model that has minimal cross validated risk 
\[ \Hat{\g}^{SL} = \argmin_{\Hat{\g} \in \mathcal{M}_\g} \sum_{v = 1}^{V} P_{Q^\mathcal{V}_v} \; L_\g(\Hat{\g}(Q^\mathcal{T}_v), Q^\mathcal{V}_v)\]

This discrete superlearner model \(\Hat{\g}^{SL}\) is then fitted on the full observed data \(Q_n\) and used to estimate \(\g_0(A \mid \X)\).

In \CRANpkg{concrete}, propensity scores are by default estimated using the \CRANpkg{SuperLearner}
package \code{formatArguments(PropScoreBackend = "Superlearner")} with candidate algorithms \code{c("xgboost", "glmnet")} implemented by packages \CRANpkg{xgboost} and \CRANpkg{glmnet}. Alternatively the \CRANpkg{sl3} package can be used by specifying \code{formatArguments(PropScoreBackend = "sl3")}.

\paragraph{Estimating Event and Censoring Hazards}
\label{HazardEstimation}
Let \(\lambda_{0,\,\delta}\) be the true censoring and cause-specific hazards when \(\delta = 0\) and \(\delta = 1, \dots, J\) respectively. Let \(\mathcal{M}_\delta\) for \(\delta = 0, \dots, J\) be the sets of candidate Cox models, \(\{\Hat{\lambda}_\delta : Q_n \to \Hat{\lambda}_\delta(Q_n)\}\), for the censoring and cause-specific hazards and let \(L_\delta\) be the log pseudo-likelihood loss function such that the risks \(\mathbb{E}_0\left[L_\delta(\Hat{\lambda}_\delta, O)\right]\) are minimized when \(\Hat{\lambda}_\delta = \lambda_{0,\,\delta}\). A discrete SuperLearner selector for each \(\delta\) chooses the candidate \(\mathcal{M_\delta}\) that has minimal cross validated risk 
\[ \Hat{\lambda}_\delta^{SL} = \argmin_{\Hat{\lambda}_\delta \in \mathcal{M}_\delta} \sum_{v = 1}^{V} P_{Q^\mathcal{V}_v} \; L_\g(\Hat{\lambda}_\delta(Q^\mathcal{T}_v), Q^\mathcal{V}_v)\]

These discrete superlearner selections \(\Hat{\lambda}_\delta^{SL}\) are then fitted on the full observed data \(Q_n\) and used to estimate \(\lambda_\delta(t \AX), \, F_\delta(t \AX),\, S(t \AX), \text{ and } S_c(t\text{-} \AX)\) for \(j = 1,\dots, J\).

For estimating the necessary conditional hazards, \CRANpkg{concrete} currently relies on a discrete Superlearner consisting of a library of Cox models implemented by \code{survival::coxph()} evaluated on cross-validated pseuo-likelihood loss. Support for estimation of hazards using Poisson-HAL or other methods may be added in the future, but currently the \code{HazEstBackend} argument must be "coxph". The default Cox specifications are a treatment-only model and a main-terms model with treatment and all covariates.  

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
DefaultHazardModels <- list("model1" = "~ trt", 
			    "model2" = "~ .")
\end{lstlisting}

\paragraph{One-step TMLE Specification}
\label{tmle-specification}
The one-step TMLE implemented in \CRANpkg{concrete} can jointly target survival and multiple cause-specific risks at multiple time points up to full curves, producing monotonic curves that sum appropriately to 1 while allowing for simultaneous inference. It does so by updating the cause-specific hazards along the universal least favorable submodel described in \cite{rytgaard_one-step_2021} and summarized briefly here in Equations \eqref{onestep} and \eqref{one-step-stop} in Section \ref{estimation}

The value of \(\epsilon\) is provided by the user as input into the argument \code{OneStepEps}; its default value is 0.1 and user-provided values must be between 0 and 1. The value of \code{OneStepEps} is meant to be heuristically small as Equation \eqref{onestep} approximates an integral; therefore it is shrunk by a factor of 2 whenever an update step would increase the norm of the efficient influence function.

To ensure that the update step does not continue infinitely, the user can use the argument \code{formatArguments(MaxUpdateIter= )} to set the maximum number of small update recursions, i.e. \(i\) for \(\epsilon_i\) in Equation \eqref{one-step-stop}. This argument takes positive integers and is set to a default of 100.

The argument \code{formatArguments(MinNuisance= )} can be used to specify a lower bound for the product of the propensity score and lagged survival probablity for remaining uncensored; this term is present in the denominator of the efficient influence function and enforcing a lower bound decreases estimator variance at the cost of introducing bias. 

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", 
				Intervention = 0:1, TargetEvent = 1:2, TargetTime = 90 * (16:24), 
				CVArg = NULL, Model = NULL, 
				PropScoreBackend = "SuperLearner", HazEstBackend = "coxph", 
				MaxUpdateIter = 100, OneStepEps = 0.1, MinNuisance = 0.05)
\end{lstlisting}


\paragraph{ConcreteArgs object}
\label{concreteargs}
\code{formatArguments()} returns a list object of class \code{"ConcreteArgs"}. This object can be modified by the user and then passed back through \code{formatArguments()} in lieu of supplying new inputs directly as separate arguments into \code{formatArguments()}. 

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteArgs <- formatArguments(DataTable = obs, EventTime = "time", EventType = "status", 
				Treatment = "trt", ID = "id", 
				Intervention = 0:1, TargetEvent = 1:2, TargetTime = 90 * (16:24), 
				CVArg = NULL, Model = ConcreteArgs[["Model"]], 
				PropScoreBackend = "SuperLearner", HazEstBackend = "coxph", 
				MaxUpdateIter = 100, OneStepEps = 1, MinNuisance = 0.05)

ConcreteArgs <- formatArguments(ConcreteArgs)
\end{lstlisting}

\subsection{doConcrete}
\label{doConcrete}
Once \code{formatArguments()} runs without errors, the resulting object of class \code{"ConcreteArgs"} should be a suitable input into the function \code{doConcrete()}. \code{doConcrete()} will then perform the specified estimation algorithm and output an object of class \code{"ConcreteEst"} which will contain contains TMLE point estimates and influence curves for the cause-specific absolute risks for each targeted event at each targeted time. If \code{formatArguments(GComp=TRUE)}, then the \code{"ConcreteEst"} object will also contain the result of using the Superlearner predictions as a plug-in g-formula estimate of the targeted risks.

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteEst <- doConcrete(ConcreteArgs)
\end{lstlisting}

For a detailed and precise description of the one-step TMLE for continuous-time survival, see \cite{rytgaard_one-step_2021} and \cite{rytgaard_continuous-time_2021}. This manuscript briefly reviews the one-step TMLE for continuous-time survival in Section \ref{estimation} and details the required steps in Section \ref{EstimationSpec}, subsections \ref{CV} through \ref{tmle-specification}. Here we will list the functions called by \code{doConcrete()} which perform each of the steps in performing the one-step continuous-time survival TMLE.

The cross-validation scheme (Section \ref{CV}) is checked by \code{formatArguments()} and if possible evaluated, outputting fold assignments as \code{.[["CVFolds"]]} of the \code{"ConcreteArgs"} object.

The initial estimation of nuisance parameters (Section \ref{NuisanceEstimation}) is performed by the function \code{concrete:::getInitialEstimate()}, with propensity estimation (Section \ref{PropScore}) performed by \code{concrete:::getPropScore()} and hazard estimation (Section \ref{HazardEstimation}) performed by \code{concrete:::getHazEstimate()}.

The one-step TMLE update procedure (Sections \ref{estimation} and \ref{tmle-specification}, Equations \eqref{eic}, \eqref{clevcov}, \eqref{onestep}, and \eqref{one-step-stop}) is performed by \code{concrete:::doTmleUpdate()} with \code{concrete:::getEIC()} computing the efficient influence curves \eqref{eic}.

\subsection{getOutput}
\label{getoutput}
\code{getOutput()} takes as an argument the \code{"ConcreteEst"} object returned by \code{doConcrete()} and returns tables and plots of the cause-specific risks, risk differences, and/or relative risks.

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none,otherkeywords={}, deletekeywords={}}
\begin{lstlisting}
ConcreteOut <- getOutput(ConcreteEst)$RD
head(ConcreteOut, 10)
\end{lstlisting}

\begin{center}
\begin{tabular}{lrrrr}
Estimator & Event & Time & RD & se\\
\hline
tmle & 1 & 1440 & -0.012989172 & 0.01786192\\
tmle & 2 & 1440 & 0.046210334 & 0.04154612\\
tmle & 1 & 1530 & 0.004761007 & 0.02131398\\
tmle & 2 & 1530 & 0.048368036 & 0.04239952\\
tmle & 1 & 1620 & 0.017551051 & 0.02310111\\
tmle & 2 & 1620 & 0.045792211 & 0.04269689\\
tmle & 1 & 1710 & 0.017551051 & 0.02310111\\
tmle & 2 & 1710 & 0.018490620 & 0.04338313\\
tmle & 1 & 1800 & 0.017551051 & 0.02310111\\
tmle & 2 & 1800 & 0.012207170 & 0.04385966\\
\end{tabular}
\end{center}


\begin{figure}[H]
\includegraphics[width=0.9\linewidth]{concrete-pbc.png}
\end{figure}

\section{simulations?}
\label{sec:org44b4fd2}
\subsection{concrete performance (vs SuperLearner g-formula and survtmle on discretized data)?}
\label{sec:orgddbb103}
\subsection{concrete performance with bad specification choices?}
\label{sec:org30560a3}
\subsection{????}
\label{sec:orga18285f}

\section{Appendix 2: Nice to have Concepts}
\label{sec:orgb2cde63}

\subsection{Identification}
\label{identification}
In order to identify causal estimands such as absolute risk ratios and differences with functions of the observed data, some untestable structural assumptions must hold - namely the assumptions of consistency, positivity, randomization, and coarsening at random on the conditional density of the censoring mechanism. 


\begin{enumerate}
\item The consistency assumption states that the observed outcome given a certain treatment decision is equal to the corresponding counterfactual outcome
\end{enumerate}
\[ T^d_j = T_j \text{ on the event that A = d(L)} \]

\begin{enumerate}
\item The positivity assumption states that the desired treatment regimes occur with non-zero probability in all observed covariate strata, and that remaining uncensored occurs with non-zero probability in all observed covariate strata at all times of interest \(t\).
\end{enumerate}
\[ P_0\left( A = d(L) \mid \L \right) > 0 \;,\, a.e. \]
\[ P(C \geq t \mid a, \L) \;,\, a.e. \]

\begin{enumerate}
\item The randomization assumption states that there is no unmeasured confounding between treatment and counterfactual outcomes
\end{enumerate}
\[ A \indep (T^d_1, T^d_2) \mid \L \]

\begin{enumerate}
\item Coarsening at random on censoring
\end{enumerate}
\[ C \indep (T^d_1, T^d_2) \mid T > C, A, \L \]

Given coarsening at random, the observed data distribution factorizes 
\begin{align*}
p_0(O) = p_{0}(\L)\, \g_0(A \mid \L)\, \lambda_{0,c}&(\T \AX)^{\1(\Delta = 0)} S_{0, c}(\T\text{-} \AX)\\
&\prod_{j=1}^{J} S_{0}(\T\text{-} \AX) \, \lambda_{0,j}(\T \AX)^{\1(\Delta = j)}
\end{align*}
where \(\lambda_{0,c}(t \AX)\) is the true cause-specific hazard of the censoring process and \(\lambda_{0,j}(t \AX)\) is the true cause-specific hazard of the \(j^{th}\) event process. Additionally
\begin{align*}
    S_{0,c}(t \ax) &= \exp\left(-\int_{0}^{t} \lambda_{0,c}(s \ax) \,ds\right)
\intertext{while in a pure competing risks setting}
    S_0(t \ax) &= \exp\left(-\int_{0}^{t} \sum_{j=1}^{J} \lambda_{0,j}(s \ax) \,ds\right)
\intertext{and} 
    F_{0,j}(t \ax) &= \int_{0}^{t} S(s\text{-} \ax) \lambda_{0,j}(s \ax)\,ds\\
    &= \int_{0}^{t} \exp\bigg(-\int_{0}^{s} \sum_{j=1}^{J} \lambda_{0,j}(u \ax)\,du\bigg) \lambda_{0,j}(s \ax)\,ds.
\end{align*}

Under the above identification assumptions, the post-intervention distribution of \(O\) under intervention \(A=d(a, \l)\) in the world of no-censoring, i.e the distribution of \((\L,\, T^d_j,\, \Delta^d_j :\, j = 1, \dots, J)\), can be represented by the so-called G-computation formula. Letâ€™s denote this post-intervention probability distribution with \(P_{d}\) and the corresponding post-intervention random variable with \(O_d\). The probability density of \(O_d\) follows from replacing \(\g_0(A \mid \L)\) with the density that results from setting \(A = d(a, l)\), \(\g_d(d(A, \l) \mid \L)\), and replacing the conditional probability of being censored at time \(t\) by no censoring with probability \(1\). In notation, \(P(O_d = o)\) is given by
\begin{align*}
p_{d}(o) = p_{0}(\l) \, &\g_d(d(a, \l) \mid \l) \, \1(\delta \neq 0)\\
&\prod_{j=1}^{J} \left[S_{0}(\t\text{-} \mid A = d(a, \l),\, \l) \, \lambda_{0,j}(\t \mid A = d(a, \l), \l)^{\1(\delta = j)} \right]
\end{align*}
Recalling the censoring and cause-specific conditional hazards defined above in terms of observed data, we should note that given the identifiability assumptions they now identify their counterfactual counterparts, i.e. 
\[\lambda_{c}(t \mid W,\, A) = \lim_{h \to 0}P(C < t + h \mid C \geq t,\, W,\, A)\]
\[\lambda_{j}(t \mid W,\, A)= \lim_{h \to 0}P(T < t+h, J=j \mid T \geq t, W, A)\]
Note that the cause-specific event hazards are not conditional on censoring once identifiability assumptions are met.

Since the density \(P(O_d=o)\) implies any probability event about \(O_d\), this g-computation formula for \(P(O_d=o)\) also implies g-computation formulas for causal quantities such as event-free survival and cause-\(k\) absolute risk under intervention \(d\). 

\newpage

\bibliography{main.bib}
\end{document}